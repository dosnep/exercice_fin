{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb50e3-b2ec-410c-8446-0dbddd92df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import duckdb\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9625e7-b480-4041-ae13-8fe074b19226",
   "metadata": {},
   "source": [
    "### Notebook Goal\n",
    "> This notebook aims to be a first draft of algorithm to detech fraudulent transactions\n",
    "\n",
    "- Data Historic:\n",
    "  - sources: transactions & transactions_label  \n",
    "  - ds_1: denormalized_table joinning transactions & transactions_label\n",
    "  - ds_2: cleanned data to only keep rows where label <=1 and drop column amount\n",
    "  - ds_3: add new column that describes if a user did a fraudulent transaction in the past\n",
    "  - ds_4: Cast created_at column in int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8343c2-c96e-4692-b17e-38f89e5bfb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transactions Tables\n",
    "Describes every transactions performed\n",
    "\"\"\"\n",
    "transactions = duckdb.sql(\"SELECT * FROM 'data/transactions.csv'\")\n",
    "transactions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b955d9-4702-4c46-8748-228ff306c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transactions Labels Tables\n",
    "Describes for every transactions if it is fraudulent (1) or not (0)\n",
    "\"\"\"\n",
    "transactions_label = duckdb.sql(\"SELECT * FROM 'data/transactions_label.csv'\")\n",
    "transactions_label.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58aa7e7-5202-4037-9357-1617a8a32970",
   "metadata": {},
   "source": [
    "### Transformation | Denormalization\n",
    "**Exercice:** Using an SQL query, construct a new table that allows containing all elements from the **transactions** table with the associated label (denormalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2028ce4-43d9-4792-aa4c-4bf7c3832ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"!!!!TO OVERRIDE!!!!\"\n",
    "\n",
    "ds_1 = duckdb.sql(input_query)\n",
    "ds_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a80201-f26c-4040-9cfe-2fbf11f4f52d",
   "metadata": {},
   "source": [
    "### Transformation | Cleaning\n",
    "**Exercise:** Using an SQL query, construct a new table that keeps only the labels <= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cca166-7764-449d-b39c-f7a4ce229ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"!!!!TO OVERRIDE!!!!\"\n",
    "\n",
    "ds_2 = duckdb.sql(input_query)\n",
    "ds_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258567f-fb9e-4792-9cea-e4b83fd5e133",
   "metadata": {},
   "source": [
    "### Transformation | Feature Engineering\n",
    "**Exercice:** We assume that if a user has made a fraudulent transaction in the past, there is a high probability that they will make new fraudulent transactions. Therefore, we want to add a new column, user_fraudulent, to the previous table using an SQL query. This column returns 1 if the user has made a fraudulent transaction in the past, and 0 otherwise. Tips: We can break down the problem into two sub-problems:\n",
    "\n",
    "1. Calculate the cumulative sum of the number of fraudulent transactions for each user over time.\n",
    "2. If the cumulative sum > 0, then the user is suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c5eae-ada5-466f-9fd4-28486524621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"!!!!TO OVERRIDE!!!!\"\n",
    "\n",
    "ds_3 = duckdb.sql(input_query)\n",
    "ds_3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66750ad6-908a-4e4e-87c7-179bfc5420da",
   "metadata": {},
   "source": [
    "### Transformation | Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc049d56-cc56-422a-a7fe-097b6f0422eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"SELECT user_id, transaction_id, user_fraudulent, epoch_ms(created_at) as created_at, label \\\n",
    "                   FROM ds_3\"\n",
    "\n",
    "ds_4 = duckdb.sql(input_query)\n",
    "ds_4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cced93-2b0c-44e7-a144-057c5ece1f5a",
   "metadata": {},
   "source": [
    "### Training | Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa9427-6f68-4b5b-827c-0ff597783595",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ds_4.df(), test_size=0.30)\n",
    "train_x = train.loc[:, ['user_id', 'transaction_id', 'created_at', 'user_fraudulent']]\n",
    "train_y = train.loc[:, ['label']]\n",
    "test_x = test.loc[:, ['user_id', 'transaction_id', 'created_at', 'user_fraudulent']]\n",
    "test_y = test.loc[:, ['label']]\n",
    "print(f\"train_size: {train_x.size}, test_size: {test_x.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393da92-7a97-4d5c-abeb-1974f05f89d5",
   "metadata": {},
   "source": [
    "### Training | Plotting labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f7149-fe22-4938-91f1-ef3af7f37478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train_x : {train_x.shape}, Test_x : {test_x.shape}\\nTrain_y : {train_y.shape}, Test_y : {test_y.shape}\")\n",
    "figure = plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(121)\n",
    "train_y['label'].hist()\n",
    "plt.title(\"Train Label Distribution\")\n",
    "ax.grid(True)\n",
    "ax = plt.subplot(122)\n",
    "test_y['label'].hist()\n",
    "plt.title(\"Test Label Distribution\")\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4730b5f-e2fd-4d73-95a1-9ce351121a2a",
   "metadata": {},
   "source": [
    "### Training | Prepare Pipeline\n",
    "**Exercice:** Modify the following configuration to test a new classifier: **[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)** with the hyperparameter **criterion** varying in the set **(\"gini\", \"entropy\", \"log_loss\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237c63d-f3f3-472b-bdb9-de06b32185eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build all pipelines\n",
    "all_models_name = [\"Logistic Regression\", \"DecisionTreeClassifier\"]\n",
    "all_models_parameters = []\n",
    "all_models_parameters.append(\n",
    "{\n",
    "    'pipeline' : Pipeline([\n",
    "        ('Standardization', StandardScaler()),\n",
    "        ('LogisticRegression', LogisticRegression())\n",
    "    ]),\n",
    "    'parameters' : [\n",
    "        {\n",
    "            'LogisticRegression__max_iter' : range(40, 50)\n",
    "        }\n",
    "    ],\n",
    "    \"train_x\" : train_x,\n",
    "    \"train_y\" : train_y,\n",
    "    \"test_x\" : test_x,\n",
    "    \"test_y\" : test_y,\n",
    "    \n",
    "    \"scores_names\" : [\"Accuracy\"], \n",
    "    \"scores_obj\" : [accuracy_score]  \n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c7b66-7212-47fa-bd03-fe70d37ed991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_best_optimized_model(pipeline, parameters, train_x, train_y, test_x, test_y, scores_names, scores_obj):\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid=parameters).fit(train_x, train_y.values.ravel())\n",
    "    evaluation_scores = return_evaluation_scores(gridsearch, scores_names, scores_obj, test_x, test_y.values.ravel())\n",
    "    return gridsearch, evaluation_scores\n",
    "\n",
    "def return_evaluation_scores(current_pipeline, scores_names, scores_obj, test_x, test_y):\n",
    "    results = {}\n",
    "    for name, obj in zip(scores_names, scores_obj):\n",
    "        results[name] = obj(current_pipeline.predict(test_x), test_y)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6ff8a-28c7-4b9b-99e9-ab2ab5865e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bests_models = []\n",
    "#Find bests Hyperparams for each models\n",
    "for name, model in zip(all_models_name, all_models_parameters):\n",
    "    print(f\"Finding best hyper parameters for model:{name}\")\n",
    "    all_bests_models.append(return_best_optimized_model(**model))\n",
    "print(f\"Models with its best hyper-parameters and associated test evaluations: {all_bests_models}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
